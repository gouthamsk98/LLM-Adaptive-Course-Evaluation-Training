{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3IovaXt4ZJQ_"
   },
   "outputs": [],
   "source": [
    "!pip install transformers trl accelerate torch bitsandbytes peft sentencepiece wandb datasets -qU \n",
    "!pip install huggingface-hub -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2d8bfe57adb4691a4d1845266371ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "import wandb\n",
    "notebook_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ck_5U-vIq7zW"
   },
   "source": [
    "#### Load HF Dataset\n",
    "\n",
    "First things first, we need to load our `mosaicml/instruct-v3` dataset. It's a great collection of effective and safe tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "def create_text_row(data):\n",
    "    if(input==None):\n",
    "        text_row = f\"\"\"<s>[INST]{data['instruction']}[/INST]\\\\n{data['output']}</s>\"\"\"\n",
    "    else :\n",
    "        text_row = f\"\"\"<s>[INST]{data['instruction']} with {data['input']} [/INST] \\\\n {data['output']}</s>\"\"\"\n",
    "    return text_row\n",
    "\n",
    "def prepare_train_data(data_id):\n",
    "    data = load_dataset(data_id, split=\"train\")\n",
    "    data_df = data.to_pandas() \n",
    "    data_df[\"text\"] =data_df.apply(create_text_row, axis =1) \n",
    "    data = Dataset.from_pandas(data_df)\n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "DI1G_zS8PWh9"
   },
   "outputs": [],
   "source": [
    "\n",
    "instruct_tune_dataset = prepare_train_data(\"gouthamsk/embedded_dataset_mixed_small\")\n",
    "instruct_tune_dataset = instruct_tune_dataset.shuffle(seed=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmAvZLhkrY6p"
   },
   "source": [
    "Let's take a peek at our dataset.\n",
    "\n",
    "It's our job to merge these `prompt` and `response` columns into a single formatted prompt for instruct-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YfDpPbAyTHff",
    "outputId": "cbd00ff9-d41f-44a4-c13d-90dc667e1de3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'output', 'instruction', 'text'],\n",
       "    num_rows: 281\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruct_tune_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': None,\n",
       " 'output': '#include <reg51.h>\\nsbit inbit= P1^0;\\nsbit outbit= P2^7;\\nbit membit;\\nvoid main(void)\\n{\\n    while(1) { //repeat forever\\n      membit= inbit;\\n      outbit= membit\\n    }\\n}',\n",
       " 'instruction': 'Write an 8051 C program to get the status of bit P1.0, save it, and send it to P2.7 continuously.',\n",
       " 'text': '<s>[INST]Write an 8051 C program to get the status of bit P1.0, save it, and send it to P2.7 continuously. here are the inputs None [/INST] \\n #include <reg51.h>\\nsbit inbit= P1^0;\\nsbit outbit= P2^7;\\nbit membit;\\nvoid main(void)\\n{\\n    while(1) { //repeat forever\\n      membit= inbit;\\n      outbit= membit\\n    }\\n}</s>'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruct_tune_dataset[280]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HNKsQDzlilSi"
   },
   "source": [
    "We're going to train on a small subset of the data - if you were considering an Epoch based approach this would reduce the amount of time spent training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qjy9PYUaY3W"
   },
   "source": [
    "### Loading the Base Model\n",
    "\n",
    "We're going to load our model in `4bit`, with double quantization, with `bfloat16` as our compute dtype.\n",
    "\n",
    "You'll notice we're loading the instruct-tuned model - this is because it's already adept at following tasks - we're just teaching it a new one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "c86037f3cfb2456f96f45efb0ad1401a",
      "59708ffcfffa4e7bb52c12fe67926db2",
      "7e753b6afa6b49faa0196334e0d1d6e9",
      "73986105015848b7b95d12799fabde7c",
      "8ae2f7d4fe7543509fd081d1a9aae78d",
      "feb46e0f4d72487ea57d9a7a8687049b",
      "065f05ea173e4fb8a7adcb70a0e7b172",
      "2927340f52b445ab992a4015e57ef97a",
      "41959027a88640a983196fa4fd9af9e1",
      "1ba06d4e9b6043478144f7060d3ac7fc",
      "706122ff04104f709a77367096db8092"
     ]
    },
    "id": "DLqufv7gaaOz",
    "outputId": "c4e10561-a6c0-4679-d0fc-a6a72f533fa9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e865b00706804b8ba0af12e587786bda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c355d663aec842d787655aabe0011d2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bea93b89c04143448f07c2a833dcce48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78cec460ca7b4f8ab9527ca201702df3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "206af5bf5dff4e5fbf99b63804e2cb16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    device_map='auto',\n",
    "    quantization_config=nf4_config,\n",
    "    use_cache=False\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dlQxPyiKi51k"
   },
   "source": [
    "Let's example how well the model does at this task currently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "wRWhXzXrc41b"
   },
   "outputs": [],
   "source": [
    "def generate_response(prompt, model):\n",
    "  encoded_input = tokenizer(prompt,  return_tensors=\"pt\", add_special_tokens=True)\n",
    "  model_inputs = encoded_input.to('cuda')\n",
    "\n",
    "  generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "  decoded_output = tokenizer.batch_decode(generated_ids)\n",
    "\n",
    "  return decoded_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "JF7ISvKjdA6h",
    "outputId": "692daecb-2301-4fd6-bf41-aa1fe1b7eca0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> [INST]Develop a C program for a ESP32 microcontroller to blink led in GPIO Pin with specific second delay[/INST] Here is a simple C program for an ESP32 microcontroller to blink an LED connected to a specific GPIO pin with a specified second delay. This program uses the FreeRTOS real-time operating system, which is often used with the ESP32. If you are not using FreeRTOS, you may need to modify the code to fit your specific setup.\\n\\n1. First, install the necessary components for the ESP32, including the Espressif IoT Development Framework and the FreeRTOS build environment.\\n\\n2. Create a new C file named `main.c` with the following code:\\n\\n```c\\n#include \"esp_system.h\"\\n#include \"esp_log.h\"\\n#include \" freertos.h\"\\n#include \" task.h\"\\n\\n#define LED_GPIO 5 // Change this to the GPIO pin connected to your LED\\n\\nvoid led_task(void *arg);\\n\\nvoid app_main()\\n{\\n    ESP_LOGI(\"Main\", \"Starting application...\");\\n\\n    // Initialize the LED GPIO as output\\n    gpio_set_level(LED_GPIO, 1);\\n    gpio_set_direction(LED_GPIO, GPIO_MODE_OUTPUT);\\n\\n    // Create the LED task\\n    xTaskCreate(led_task, \"led_task\", 2048, NULL, 1, NULL);\\n\\n    // Sync through initialization and start the scheduler\\n    vTaskDelay(pdmsToTicks(1000)); // Delay for 1 second\\n    vTaskStartScheduler();\\n}\\n\\nvoid led_task(void *arg)\\n{\\n    while (1)\\n    {\\n        // Turn on LED\\n        gpio_set_level(LED_GPIO, 1);\\n        vTaskDelay(pdmsToTicks(1000)); // Delay for 1 second\\n\\n        // Turn off LED\\n        gpio_set_level(LED_GPIO, 0);\\n        vTaskDelay(pdmsToTicks(1000)); // Delay for 1 second\\n    }\\n}\\n```\\n\\n3. Replace `LED_GPIO 5` with the GPIO pin number connected to your LED.\\n\\n4. Compile and run your code using the `idf.py` command line tool. If you\\'re using the Arduino IDE or other development environments, you may need to modify the code accordingly to generate a valid sketch for an ESP32.\\n\\n5. Once the code runs successfully, your LED should start blinking with a 1 second on-delay and a 1 second off-delay.</s>'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_response(\"[INST]Develop a C program for a ESP32 microcontroller to blink led in GPIO Pin with specific second delay[/INST]\",model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Mar  2 10:13:25 2024       \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 535.86.10              Driver Version: 535.86.10    CUDA Version: 12.2     |\r\n",
      "|-----------------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                      |               MIG M. |\r\n",
      "|=========================================+======================+======================|\r\n",
      "|   0  NVIDIA A100-SXM4-40GB          On  | 00000000:00:04.0 Off |                    0 |\r\n",
      "| N/A   33C    P0              54W / 400W |  10135MiB / 40960MiB |      0%      Default |\r\n",
      "|                                         |                      |             Disabled |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                            |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
      "|        ID   ID                                                             Usage      |\r\n",
      "|=======================================================================================|\r\n",
      "|    0   N/A  N/A     12701      C   /opt/conda/bin/python                     10122MiB |\r\n",
      "+---------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KB5_6wVYjDDu"
   },
   "source": [
    "Now, we're going to prepare our model for 4bit LoRA training!\n",
    "\n",
    "We can use these handy helper functions to achieve this goal thanks to `huggingface` and the `peft` library!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "SQulqDzjd0gD"
   },
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM, LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "bcco3ITVd486"
   },
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "go8_nVq7jO9d"
   },
   "source": [
    "All that's left to do is set up a number of hyper parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "YhoCjs9md8pB"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "  output_dir = \"mistral_embedded_c_v0.2\",\n",
    "  #num_train_epochs=10,\n",
    "  max_steps = 400, # comment out this line if you want to train in epochs\n",
    "  per_device_train_batch_size = 5,\n",
    "  warmup_steps = 0.03,\n",
    "  logging_steps=10,\n",
    "  save_strategy=\"epoch\",\n",
    "  #evaluation_strategy=\"epoch\",\n",
    "  # evaluation_strategy=\"steps\",\n",
    "  # eval_steps=20, # comment out this line if you want to evaluate at the end of each epoch\n",
    "  learning_rate=2e-4,\n",
    "  bf16=True,\n",
    "  lr_scheduler_type='constant',\n",
    "  report_to=\"wandb\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UyyNtDrmeAkc",
    "outputId": "6f990d2f-48be-4fc9-f64b-31519405d49b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d287e387dbd64c69bd0f2378db81b082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "max_seq_length = 2048\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "  model=model,\n",
    "  peft_config=peft_config,\n",
    "  max_seq_length=max_seq_length,\n",
    "  tokenizer=tokenizer,\n",
    "  packing=True,\n",
    "  # formatting_func=create_prompt,\n",
    "  args=args,\n",
    "  dataset_text_field=\"text\",\n",
    "  train_dataset=instruct_tune_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "bsOO4bR9fQBb",
    "outputId": "b544a5bb-fa4e-4b91-93e3-4f241db9281f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='70' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [70/70 03:30, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.290500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.796200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.699900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.640700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.538800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.475300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.420900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=70, training_loss=0.6946241174425397, metrics={'train_runtime': 213.6705, 'train_samples_per_second': 1.31, 'train_steps_per_second': 0.328, 'total_flos': 2.455902363844608e+16, 'train_loss': 0.6946241174425397, 'epoch': 10.0})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model=\"gemma_embedded_c_7b\"\n",
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "merged_model= PeftModel.from_pretrained(base_model, new_model)\n",
    "merged_model= merged_model.merge_and_unload()\n",
    "\n",
    "# Save the merged model\n",
    "merged_model.save_pretrained(\"merged_model\",safe_serialization=True)\n",
    "tokenizer.save_pretrained(\"merged_model\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model.push_to_hub(new_model, use_temp_dir=False)\n",
    "tokenizer.push_to_hub(new_model, use_temp_dir=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "ysHgWnwbfRSt"
   },
   "outputs": [],
   "source": [
    "trainer.save_model(\"mistral_embedded_c_v0.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "na6xC6f-mGGm"
   },
   "source": [
    "# Save Model and Push to Hub\n",
    "\n",
    "4bit save and push coming soon!\n",
    "\n",
    "The PR is literally in the process of being added! Check it out [here](https://github.com/TimDettmers/bitsandbytes/pull/753)!\n",
    "\n",
    "For now, we'll save our adapters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "HJTZaSXqnqPa"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a3b53a5657749fb92b78008a4b31f49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d60ce3131b1b4f55bc4598f53b836157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "events.out.tfevents.1709374432.sky-01ae-biboxdev-1cea-head-c28bxh13-compute:   0%|          | 0.00/6.80k [00:0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0595f98d2600464082ee974a8900246a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/109M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "160ce9baeda5476c8109672de794a0fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/4.92k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/gouthamsk/mistral_embedded_c/commit/d25e4b4ae4e5893de854e4090289f87b745c7366', commit_message='gouthamsk/mistral_embedded_c', commit_description='', oid='d25e4b4ae4e5893de854e4090289f87b745c7366', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub(\"gouthamsk/mistral_embedded_c_v0.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9zYrr6sfkA6M",
    "outputId": "8bb104a9-a7d3-4e39-f563-12edb49a8ccb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:272: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "merged_model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "c1Eo6D2mnOgN"
   },
   "outputs": [],
   "source": [
    "def generate_response(prompt, model):\n",
    "  encoded_input = tokenizer(prompt,  return_tensors=\"pt\", add_special_tokens=True)\n",
    "  model_inputs = encoded_input.to('cuda')\n",
    "\n",
    "  generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "  decoded_output = tokenizer.batch_decode(generated_ids)\n",
    "\n",
    "  return decoded_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "0FcXIJSLn_TK",
    "outputId": "e33ec0e3-161d-4334-ca4c-3d561bd427f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> [INST]Is the bellow answer correct answer to \"the program to blink LED in GPIO PIN 2 with 1 second delay(1 second on and 1 second off)\"\\n//user input answer start\\n#include <stdio.h>\\n#include \"freertos/FreeRTOS.h\"\\n#include \"freertos/task.h\"\\n#include <driver/gpio.h>\\n#define LED_PIN 2\\nvoid ledON(){\\n    gpio_set_level(1, 0);\\n}\\nvoid ledOFF(){\\n    gpio_set_level(2, 0);\\n}\\nvoid app_main(){\\n    gpio_config_t io_conf = {\\n        .pin_bit_mask = (1ULL<<3),\\n        .mode = GPIO_MODE_OUTPUT,\\n    };\\n    gpio_config(&io_conf);\\n    while (1) {\\n        ledON();\\n        vTaskDelay(1000 / portTICK_PERIOD_MS);\\n        ledOFF();\\n        vTaskDelay(1000 / portTICK_PERIOD_MS);\\n    }\\n}\\n//user input answer end\\n[/INST] Yes, the provided answer code appears to correctly implement a program to blink an LED connected to GPIO Pin 2 with a 1 second delay (on for 1 second and off for 1 second). The code uses the FreeRTOS library and the driver/gpio library to configure the GPIO pin as an output and to toggle the LED state. The while loop with vTaskDelay calls is used to control the on/off states of the LED with the desired delay.</s>'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_response(\"\"\"[INST]Is the bellow answer correct answer to \"the program to blink LED in GPIO PIN 2 with 1 second delay(1 second on and 1 second off)\"\n",
    "//user input answer start\n",
    "#include <stdio.h>\n",
    "#include \"freertos/FreeRTOS.h\"\n",
    "#include \"freertos/task.h\"\n",
    "#include <driver/gpio.h>\n",
    "#define LED_PIN 2\n",
    "void ledON(){\n",
    "    gpio_set_level(1, 0);\n",
    "}\n",
    "void ledOFF(){\n",
    "    gpio_set_level(2, 0);\n",
    "}\n",
    "void app_main(){\n",
    "    gpio_config_t io_conf = {\n",
    "        .pin_bit_mask = (1ULL<<3),\n",
    "        .mode = GPIO_MODE_OUTPUT,\n",
    "    };\n",
    "    gpio_config(&io_conf);\n",
    "    while (1) {\n",
    "        ledON();\n",
    "        vTaskDelay(1000 / portTICK_PERIOD_MS);\n",
    "        ledOFF();\n",
    "        vTaskDelay(1000 / portTICK_PERIOD_MS);\n",
    "    }\n",
    "}\n",
    "//user input answer end\n",
    "[/INST]\"\"\",merged_model)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "065f05ea173e4fb8a7adcb70a0e7b172": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1ba06d4e9b6043478144f7060d3ac7fc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2927340f52b445ab992a4015e57ef97a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "41959027a88640a983196fa4fd9af9e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "59708ffcfffa4e7bb52c12fe67926db2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_feb46e0f4d72487ea57d9a7a8687049b",
      "placeholder": "​",
      "style": "IPY_MODEL_065f05ea173e4fb8a7adcb70a0e7b172",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "706122ff04104f709a77367096db8092": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "73986105015848b7b95d12799fabde7c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1ba06d4e9b6043478144f7060d3ac7fc",
      "placeholder": "​",
      "style": "IPY_MODEL_706122ff04104f709a77367096db8092",
      "value": " 2/2 [00:15&lt;00:00,  7.16s/it]"
     }
    },
    "7e753b6afa6b49faa0196334e0d1d6e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2927340f52b445ab992a4015e57ef97a",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_41959027a88640a983196fa4fd9af9e1",
      "value": 2
     }
    },
    "8ae2f7d4fe7543509fd081d1a9aae78d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c86037f3cfb2456f96f45efb0ad1401a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_59708ffcfffa4e7bb52c12fe67926db2",
       "IPY_MODEL_7e753b6afa6b49faa0196334e0d1d6e9",
       "IPY_MODEL_73986105015848b7b95d12799fabde7c"
      ],
      "layout": "IPY_MODEL_8ae2f7d4fe7543509fd081d1a9aae78d"
     }
    },
    "feb46e0f4d72487ea57d9a7a8687049b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
