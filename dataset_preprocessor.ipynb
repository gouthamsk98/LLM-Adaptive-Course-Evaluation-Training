{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Util Function for clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def extract_text_from_folder(folder_path,column_name=\"text\"):\n",
    "    text_data = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith(('.txt', '.rst')):\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    text = file.read()\n",
    "                    text_data.append(text)\n",
    "    text_data_df = pd.DataFrame(np.array(text_data), columns=[column_name])\n",
    "    return text_data_df\n",
    "def extract_code_from_folder(folder_path,column_name=\"text\"):\n",
    "    text_data = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith(('.h')):\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    text = file.read()\n",
    "                    text_data.append(text)\n",
    "    text_data_df = pd.DataFrame(np.array(text_data), columns=[column_name])\n",
    "    return text_data_df\n",
    "\n",
    "def clean_text_data(text_data):\n",
    "    # Remove Markdown links\n",
    "    text_data = re.sub(r'\\:link_to_translation\\:\\`[^`]*\\`', '', text_data)\n",
    "    \n",
    "    # Remove code blocks\n",
    "    text_data = re.sub(r'```.*?```', '', text_data, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text_data = re.sub(r'<[^>]+>', '', text_data)\n",
    "    \n",
    "    # Remove section headers\n",
    "    text_data = re.sub(r'^=+\\n.*\\n=+\\n', '', text_data, flags=re.MULTILINE)\n",
    "    text_data = re.sub(r'^-+\\n.*\\n-+\\n', '', text_data, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove bulleted lists\n",
    "    text_data = re.sub(r'^\\s*\\*\\s+.*\\n', '', text_data, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove numbered lists\n",
    "    text_data = re.sub(r'^\\s*\\d+\\.\\s+.*\\n', '', text_data, flags=re.MULTILINE)\n",
    "    #Remove special sequence\n",
    "    text_data=re.sub(r'====+','',text_data) \n",
    "    text_data=re.sub(r'\\^\\^\\^\\^+','',text_data) \n",
    "    text_data=re.sub(r'----+','',text_data) \n",
    "    text_data=re.sub(r'\\*\\*\\*\\*\\*+','',text_data) \n",
    "    # Remove indentation and empty lines\n",
    "    # text_data = re.sub(r'^\\s+', '', text_data, flags=re.MULTILINE)\n",
    "    # text_data = re.sub(r'^\\n', '', text_data, flags=re.MULTILINE)\n",
    "    \n",
    "    return text_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the textual data into chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import nltk.data\n",
    "\n",
    "# Load the NLTK sentence tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "\n",
    "def split_text(text, chunk_size=500):\n",
    "    # Define regex pattern for identifying sentence endings\n",
    "    sentence_endings = r'[.!?]'\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    current_length = 0\n",
    "    for char in text:\n",
    "        current_chunk += char\n",
    "        current_length += 1\n",
    "        if re.search(sentence_endings, char):\n",
    "            if current_length >= chunk_size:\n",
    "                chunks.append(current_chunk)\n",
    "                current_chunk = \"\"\n",
    "                current_length = 0\n",
    "    if current_chunk:  # Append any remaining part\n",
    "        chunks.append(current_chunk)\n",
    "    return chunks\n",
    "\n",
    "def split_dataframe_manual(df, column_name='text', chunk_size=500):\n",
    "    new_rows = []\n",
    "    for index, row in df.iterrows():\n",
    "        text = row[column_name]\n",
    "        chunks = split_text(text, chunk_size)\n",
    "        for chunk in chunks:\n",
    "            new_row = row.copy()\n",
    "            new_row[column_name] = chunk\n",
    "            new_rows.append(new_row)\n",
    "    return pd.DataFrame(new_rows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token distribution plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "base_model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def generate_and_tokenize_prompt(prompt):\n",
    "    return tokenizer(prompt[\"text\"])\n",
    "\n",
    "def plot_data_lengths(tokenized_train_dataset,column_name=\"text\"):\n",
    "    lengths = [len(x[column_name]) for x in tokenized_train_dataset]\n",
    "    # lengths += [len(x['text_data']) for x in tokenized_val_dataset]\n",
    "    print(len(lengths))\n",
    "\n",
    "    # Plotting the histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(lengths, bins=20, alpha=0.7, color='blue')\n",
    "    plt.xlabel('Length of input_ids')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Lengths of input_ids')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ESP_IDF docs cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = extract_text_from_folder(\"./raw_dataset_USFTHF/docs/\",column_name=\"text\")\n",
    "text_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = text_data.drop_duplicates()\n",
    "clean_data = clean_data.map(clean_text_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_pandas(clean_data)\n",
    "train_dataset= train_dataset.map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
    "# tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
    "plot_data_lengths(tokenized_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_data=split_dataframe_manual(clean_data,column_name=\"text\",max_chunk_size=1000,max_total_size=4000)\n",
    "split_data=split_dataframe_manual(clean_data,column_name=\"text\",chunk_size=1000)\n",
    "\n",
    "split_data\n",
    "train_dataset = Dataset.from_pandas(split_data)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
    "plot_data_lengths(tokenized_train_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove extra lengthy word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data = split_data[split_data['text'].apply(lambda x: len(str(x)) <= 4000)]\n",
    "\n",
    "train_dataset = Dataset.from_pandas(split_data)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
    "plot_data_lengths(tokenized_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.push_to_hub(\"gouthamsk/esp_idf_text\",split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleanup ESP_idf code example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_data = extract_code_from_folder(\"./raw_dataset_USFTHF/code/\",column_name=\"text\")\n",
    "code_data.to_csv('clean_data.csv', index=False)\n",
    "code_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_code = code_data.drop_duplicates()\n",
    "clean_code = code_data.map(clean_text_data)\n",
    "code_data.to_csv('clean_code.csv', index=False)\n",
    "clean_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data=split_dataframe_manual(clean_code,column_name=\"text\",chunk_size=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data = split_data[split_data['text'].apply(lambda x: len(str(x)) <= 10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_pandas(split_data)\n",
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
    "plot_data_lengths(tokenized_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset.push_to_hub(\"gouthamsk/esp_idf_code\",split=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "# Drop duplicate rows across all columns\n",
    "\n",
    "train_data, test_data = train_test_split(new_df, test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(split_train_data)\n",
    "train_dataset.push_to_hub(\"gouthamsk/esp_idf_text\",split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "# Drop duplicate rows across all columns\n",
    "new_df = new_df.drop_duplicates()\n",
    "train_data, test_data = train_test_split(new_df, test_size=0.2)\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "eval_dataset = Dataset.from_pandas(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset.push_to_hub(\"gouthamsk/esp_idf_text\",split=\"train\")\n",
    "eval_dataset.push_to_hub(\"gouthamsk/esp_idf_text\",split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset,eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling Junk Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install justext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = extract_text_from_folder(\"./raw_dataset_USFTHF/docs/\",column_name=\"text\")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import justext\n",
    "paragraphs = justext.justext(text, justext.get_stoplist(\"English\"))\n",
    "for paragraph in paragraphs:\n",
    "  if not paragraph.is_boilerplate:\n",
    "    print(paragraph.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/api-conventions.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links=[\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/api-conventions.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/protocols/mqtt.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/protocols/esp_tls.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/protocols/esp_http_client.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/protocols/esp_local_ctrl.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/protocols/esp_serial_slave_link.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/protocols/esp_crt_bundle.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/protocols/esp_http_server.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/protocols/esp_https_server.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/protocols/icmp_echo.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/protocols/mbedtls.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/bluetooth/esp_bt_defs.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/bluetooth/esp_bt_main.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/bluetooth/esp_bt_device.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/bluetooth/esp_gap_ble.html#\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/bluetooth/esp_gatt_defs.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/bluetooth/esp_gatts.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/bluetooth/esp_gattc.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/bluetooth/esp_blufi.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/bluetooth/esp_gap_bt.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/bluetooth/esp_a2dp.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/bluetooth/esp_avrc.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/bluetooth/esp_spp.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/bluetooth/esp_hf_defs.html#\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/bluetooth/esp_hf_client.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/bluetooth/esp_hf_ag.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/bluetooth/esp_hidd.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/bluetooth/esp_l2cap_bt.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/bluetooth/esp_sdp.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/bluetooth/controller_vhci.html#application-example\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/bluetooth/esp-ble-mesh.html#\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/bluetooth/nimble/index.html#threading-model\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/error-codes.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/network/esp_now.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/network/esp-wifi-mesh.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/network/esp_smartconfig.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/network/esp_dpp.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/network/esp_nan.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/network/esp_eth.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/network/esp_openthread.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/network/esp_netif.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/network/esp_netif_driver.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/peripherals/adc_oneshot.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/peripherals/adc_continuous.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/peripherals/adc_calibration.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/peripherals/clk_tree.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/peripherals/dac.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/peripherals/gpio.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/peripherals/gptimer.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/peripherals/i2c.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/peripherals/i2s.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/peripherals/lcd.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/peripherals/ledc.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/peripherals/mcpwm.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/peripherals/pcnt.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/peripherals/rmt.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/peripherals/sdspi_host.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/peripherals/sdio_slave.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/peripherals/sdm.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/peripherals/spi_flash/index.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/peripherals/spi_master.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/peripherals/spi_slave.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/peripherals/secure_element.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/peripherals/touch_pad.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/peripherals/twai.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/peripherals/uart.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/kconfig.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/provisioning/protocomm.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/provisioning/provisioning.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/provisioning/wifi_provisioning.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/storage/fatfs.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/storage/mass_mfg.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/storage/nvs_flash.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/storage/nvs_encryption.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/storage/nvs_partition_gen.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/storage/sdmmc.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/storage/partition.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/storage/spiffs.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/storage/vfs.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/storage/wear-levelling.html\"\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/system/app_image_format.html\"\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/system/bootloader_image_format.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/system/app_trace.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/system/esp_function_with_shared_stack.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/system/chip_revision.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/system/console.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/system/efuse.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/system/esp_err.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/system/esp_https_ota.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/system/esp_event.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/system/freertos.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/system/freertos_idf.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/system/freertos_additions.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/system/mem_alloc.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/system/mm.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/system/heap_debug.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/system/esp_timer.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/system/internal-unstable.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/system/ipc.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/system/intr_alloc.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/system/log.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/system/misc_system_api.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/system/ota.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/system/perfmon.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/system/power_management.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/system/pthread.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/system/random.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/system/sleep_modes.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/system/soc_caps.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/system/system_time.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/system/himem.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/system/ulp.html\",\n",
    "    \"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/system/wdts.html\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import justext\n",
    "\n",
    "response = requests.get(\"https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-reference/bluetooth/esp_gap_ble.html#\")\n",
    "paragraphs = justext.justext(response.content, justext.get_stoplist(\"English\"))\n",
    "for paragraph in paragraphs:\n",
    "  if not paragraph.is_boilerplate:\n",
    "    print(paragraph.text)\n",
    "paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lxml_html_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --force-reinstall --no-deps babel==2.13.1 \n",
    "!pip3 install cchardet  # single package only\n",
    "!pip3 install trafilatura[all] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trafilatura\n",
    "\n",
    "urls = save\n",
    "for url in urls:\n",
    "    downloaded = trafilatura.fetch_url(url)\n",
    "    if downloaded:\n",
    "        extracted_text = trafilatura.extract(downloaded)\n",
    "        print(f\"Content from {url}:\\n{extracted_text}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data.txt\"\n",
    "\n",
    "# Open the file in write mode ('w')\n",
    "with open(file_path, 'w') as file:\n",
    "    # Write the data to the file\n",
    "    file.write(data[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import justext\n",
    "\n",
    "response = requests.get(\"https://dev.ti.com/tirex/explore/content/mspm0_academy_2_01_01_00/_build_mspm0_academy_2_01_01_00/index.html\")\n",
    "paragraphs = justext.justext(response.content, justext.get_stoplist(\"English\"))\n",
    "file_path = \"data2.txt\"\n",
    "\n",
    "# Open the file in write mode ('w')\n",
    "with open(file_path, 'w') as file:\n",
    "  a=\"\"\n",
    "    # Write the data to the file\n",
    "  for paragraph in paragraphs:\n",
    "    if not paragraph.is_boilerplate:\n",
    "      file.write(paragraph.text)\n",
    "      print(paragraph.text)\n",
    "      a+=paragraph.text\n",
    "  print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install trafilatura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trafilatura.spider import focused_crawler\n",
    "\n",
    "homepage = 'https://dev.ti.com/tirex/explore/content/mspm0_academy_2_01_01_00/_build_mspm0_academy_2_01_01_00/index.html'\n",
    "# starting a crawl\n",
    "to_visit, known_urls = focused_crawler(homepage, max_seen_urls=10, max_known_urls=100000)\n",
    "# resuming a crawl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_visit, known_urls = focused_crawler(homepage, max_seen_urls=10, max_known_urls=100000, todo=to_visit, known_links=known_urls)\n",
    "print(len(to_visit), len(known_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save =known_urls\n",
    "save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "file_path = \"url_list.json\"\n",
    "with open(file_path, 'w') as file:\n",
    "    # json.dump(to_visit, file_path, indent=1) \n",
    "    for item in to_visit:\n",
    "        file.write(\"%s,\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary functions\n",
    "from trafilatura import fetch_url, extract\n",
    "import json\n",
    "\n",
    "# grab a HTML file to extract data from\n",
    "for url in save:\n",
    "    downloaded = fetch_url(url)\n",
    "    data = extract(downloaded, output_format=\"json\")\n",
    "    data = json.loads(data)\n",
    "    file_path = \"data.jsonl\"\n",
    "    # Open the file in write mode ('w')\n",
    "    with open(file_path, 'w') as file:\n",
    "        # Write the data to the file\n",
    "        file.write(data[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.2.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.6)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.27.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->datasets) (4.8.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Error parsing requirements for babel: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/Babel-2.13.1.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[115], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset, Dataset\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the JSONL file as a dataset\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# dataset = load_dataset('json', data_files='data.jsonl', split='train')\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# # Display the dataset\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# print(dataset)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/__init__.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Datasets Authors and the TensorFlow Datasets Authors.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3.2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReadInstruction\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:59\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfsspec\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpa\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompute\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpc\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/__init__.py:22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _hard_dependencies, _dependency, _missing_dependencies\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# numpy compat\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_numpy_dev \u001b[38;5;28;01mas\u001b[39;00m _is_numpy_dev  \u001b[38;5;66;03m# pyright: ignore # noqa:F401\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hashtable \u001b[38;5;28;01mas\u001b[39;00m _hashtable, lib \u001b[38;5;28;01mas\u001b[39;00m _lib, tslib \u001b[38;5;28;01mas\u001b[39;00m _tslib\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/compat/__init__.py:25\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_constants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     18\u001b[0m     IS64,\n\u001b[1;32m     19\u001b[0m     PY39,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     PYPY,\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompressors\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     26\u001b[0m     is_numpy_dev,\n\u001b[1;32m     27\u001b[0m     np_version_under1p21,\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     30\u001b[0m     pa_version_under7p0,\n\u001b[1;32m     31\u001b[0m     pa_version_under8p0,\n\u001b[1;32m     32\u001b[0m     pa_version_under9p0,\n\u001b[1;32m     33\u001b[0m     pa_version_under11p0,\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_function_name\u001b[39m(f: F, name: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mcls\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m F:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/compat/numpy/__init__.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\" support numpy compatibility across versions \"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Version\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# numpy versioning\u001b[39;00m\n\u001b[1;32m      7\u001b[0m _np_version \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39m__version__\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/util/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# pyright: reportUnusedImport = false\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decorators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     Appender,\n\u001b[1;32m      4\u001b[0m     Substitution,\n\u001b[1;32m      5\u001b[0m     cache_readonly,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhashing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     hash_array,\n\u001b[1;32m     10\u001b[0m     hash_pandas_object,\n\u001b[1;32m     11\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/util/_decorators.py:14\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     Any,\n\u001b[1;32m      8\u001b[0m     Callable,\n\u001b[1;32m      9\u001b[0m     Mapping,\n\u001b[1;32m     10\u001b[0m     cast,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproperties\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cache_readonly\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_typing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m     F,\n\u001b[1;32m     17\u001b[0m     T,\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_exceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find_stack_level\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/_libs/__init__.py:13\u001b[0m\n\u001b[1;32m      1\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaT\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaTType\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterval\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m ]\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     15\u001b[0m     NaT,\n\u001b[1;32m     16\u001b[0m     NaTType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     iNaT,\n\u001b[1;32m     22\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/_libs/interval.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.interval\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# Load the JSONL file as a dataset\n",
    "# dataset = load_dataset('json', data_files='data.jsonl', split='train')\n",
    "\n",
    "# # Display the dataset\n",
    "# print(dataset)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
